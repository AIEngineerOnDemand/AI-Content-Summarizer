{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word count: 685\n"
     ]
    }
   ],
   "source": [
    "from utils.download_content import download_from_medium\n",
    "url = \"https://dswharshit.medium.com/start-building-these-projects-to-become-an-llm-engineer-0064e9e68d9d\"\n",
    "text = download_from_medium(url)\n",
    "word_count = len(text.split())\n",
    "print(f\"Word count: {word_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\giopl\\OneDrive\\Desktop\\MetodoGiapponese\\py\\lib\\site-packages\\transformers\\utils\\generic.py:311: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "c:\\Users\\giopl\\OneDrive\\Desktop\\MetodoGiapponese\\py\\lib\\site-packages\\transformers\\utils\\generic.py:311: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "from utils.summarize_text import summarize_text,filter_information "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explained_task = \"\"\"\n",
    "# use the information in the following text to create a series of industry-focussed data science interview questions, \n",
    "# as in these examples:\n",
    "#         \"question\": \"Bonus Question: Discuss how to make your model robust to outliers\n",
    "#         \"answer\": There are several options when it comes to strengthening your model in terms of outliers. Investigating these outliers is always the first step in understanding how to treat them. After you recognize the nature of why they occurred, you can apply one of the several methods below:...,\n",
    "#         \"question\": What are the key differences between supervised and unsupervised learning?\",\n",
    "#         \"answer\": \"Supervised learning involves training a model on labeled data, where the correct output is provided for each input. Unsupervised learning, on the other hand, involves training a model on unlabeled data, where the model tries to find patterns and relationships in the data without any guidance...\"\n",
    "# \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "explained_task = \"\"\"\n",
    "First task:produce a summary of the text that makes a list of the technical tools mentioned, \n",
    "the use cases and the buisiness value mentioned. Then create a diagram in markdown to illustrate how the tools are used to build the solution for the use cases mentioned.\n",
    "Second task return only either technical tools or use cases or business value mentioned in the text and erase all other information. \n",
    "Here is the text:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = explained_task + text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'prompt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mprompt\u001b[49m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'prompt' is not defined"
     ]
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word count: 685\n"
     ]
    }
   ],
   "source": [
    "word_count = len(text.split())\n",
    "print(f\"Word count: {word_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\giopl\\OneDrive\\Desktop\\MetodoGiapponese\\py\\lib\\site-packages\\huggingface_hub\\file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered Technical Tools: \n",
      "    Return only the technical tools mentioned in the following text and erase all other information:\n",
      "    Harshit Tyagi Follow -- 16 Listen Share More “Develop a habit of working on your own projects. Don’t let “work” mean something other people tell you to do. If you do manage to do great work one day, it will probably be on a project of your own. It may be within some bigger project, but you’ll be driving your part of it.” — Paul Graham For aspiring AI professionals, becoming an LLM engineer offers an exciting and promising career path. But where should you start? How should you learn? In one of my previous posts I laid out the complete roadmap to become an AI / LLM Engineer. Reading this will give you insights on the type of skills you have to acquire and how. dswharshit.medium.com As Andrej Karpathy puts it: Work on projects. That is the best way to not just learn but really grok these concepts. It will further sharpen the skill to think about cutting edge use cases. But the main challenge with this learning philosophy is that good projects are hard to find. And that’s the problem I am trying to resolve. Helping people, including myself, discover and build practical and real-world projects that impart skills worth showcasing in your portfolio. Your project can’t be just another analysis on Titanic dataset. If you’re a beginner, your initial projects should showcase that you can comfortably build applications with LLMs i.e. Building a chatbot provides a great starting point but at this point everyone has developed one, there are many solutions for easy Streamlit based prototypes. So, you need to develop something that’s actually usable and has the potential to reach a wider audience. What does that mean? That means build a chatbot for WhatsApp or Discord or Telegram. That means build a chatbot which solves a problem people struggle with, a problem companies have started to build solutions for. If I had to pick a good and, arguably, the most common AI project that every company has started to work on is RAG-powered Chatbots. But before you get to building RAG-powered bots, you should start building something slightly more basic with LLMs. Here’s what I would do. This chatbot allows users to send a YouTube video URL via text message. It also allows users to send a text message via email. It also allows users to send a text message via email. It also allows users to send a text message via email. It also allows users to send a text message via email. It also allows users to send a text message via email. It also allows users to send a text message via email. It also allows users to send a text message via email. It also allows users to send a text message via email. It also allows users to send a text message via email. It also allows users to send a text message via email. It also allows users to send a text message via email. It also allows users to send a text message via email. It also allows\n"
     ]
    }
   ],
   "source": [
    "filtered_tools = filter_information(text, \"technical tools\")\n",
    "print(\"Filtered Technical Tools:\", filtered_tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('filtered_tools.txt', 'w') as file:\n",
    "    file.write(filtered_tools)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\giopl\\OneDrive\\Desktop\\MetodoGiapponese\\py\\lib\\site-packages\\huggingface_hub\\file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "prompt = \" produce a list of tools discussed in the following text, that  looks like this: Tool 1: TensorFlow.Tool 2: PyTorchTool 3: Scikit-learnTool 4: Keras\" + text\n",
    "summary = summarize_text(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('summary.txt', 'w') as file:\n",
    "    file.write(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\giopl\\OneDrive\\Desktop\\MetodoGiapponese\\py\\lib\\site-packages\\huggingface_hub\\file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'what is the last name of Jogn Smith?\\n\\nJogn Smith: \"Jogn\" is the last name of Jogn Smith.\\n\\nJogn Smith: \"Jogn\" is the last name of Jogn Smith.\\n\\nJogn Smith: \"Jogn\" is the last name of Jogn Smith.\\n\\nJogn Smith: \"Jogn\" is the last name of Jogn Smith.\\n\\nJogn Smith: \"Jogn\" is the last name of Jogn Smith.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarize_text(\"what is the last name of Jogn Smith?\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to gpt2 and revision 6c0e608 (https://huggingface.co/gpt2).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "c:\\Users\\giopl\\OneDrive\\Desktop\\MetodoGiapponese\\py\\lib\\site-packages\\huggingface_hub\\file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\giopl\\OneDrive\\Desktop\\MetodoGiapponese\\py\\lib\\site-packages\\transformers\\utils\\generic.py:311: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64723c50c692427f839a89d3446f0cda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\giopl\\OneDrive\\Desktop\\MetodoGiapponese\\py\\lib\\site-packages\\huggingface_hub\\file_download.py:139: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\giopl\\.cache\\huggingface\\hub\\models--gpt2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Create content generator pipeline\n",
    "\n",
    "generator = pipeline('text-generation')\n",
    "\n",
    "# Generate content based on given context\n",
    "\n",
    "output = generator('Python is a versatile programming language that', max_length=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'Python is a versatile programming language that supports a variety of languages, especially for programming based applications. We will cover the fundamentals and how to build an interpreter with a standard BASIC interpreter. Our focus will be on using Java to do things: creating an interactive interpreter, executing an automated program, and learning about Java programming via language learning methods.\\n\\nRequirements\\n\\nPython 3.3 or above and have access to the required IDE.\\n\\nPrerequisites\\n\\nYou will need the following for this'}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = generator(, max_length=100, num_return_sequences=5)\n",
    "with open('summary.txt', 'w') as file:\n",
    "    file.write(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\giopl\\OneDrive\\Desktop\\MetodoGiapponese\\py\\lib\\site-packages\\torchvision\\io\\image.py:14: UserWarning: Failed to load image Python extension: '[WinError 127] The specified procedure could not be found'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from transformers import GPTNeoForCausalLM, GPT2Tokenizer\n",
    "\n",
    "# Load the GPT-Neo model and tokenizer\n",
    "model_name = \"EleutherAI/gpt-neo-2.7B\"\n",
    "model = GPTNeoForCausalLM.from_pretrained(model_name)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Define the prompt\n",
    "prompt = \"Here is an example prompt to generate text using GPT-Neo.\"\n",
    "\n",
    "# Encode the prompt\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "# Generate text\n",
    "output = model.generate(inputs.input_ids, max_length=100, num_return_sequences=5)\n",
    "\n",
    "# Decode the generated text\n",
    "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "# Save the generated text to a file\n",
    "with open('summary.txt', 'w') as file:\n",
    "    file.write(generated_text)\n",
    "\n",
    "print(\"Generated text saved to summary.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
